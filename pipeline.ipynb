{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0f1db1",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85af83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730a4d5",
   "metadata": {},
   "source": [
    "## NLP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9fcb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\telog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7df386",
   "metadata": {},
   "source": [
    "## DAFTAR KATA PENTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877cf55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOOM_VERBS = [\n",
    "    \"analyze\", \"evaluate\", \"create\", \"design\", \"develop\", \"assess\",\n",
    "    \"compare\", \"apply\", \"interpret\", \"explain\", \"demonstrate\",\n",
    "    \"construct\", \"formulate\", \"summarize\", \"predict\"\n",
    "]\n",
    "\n",
    "THEMATIC_TERMS = [\n",
    "    \"account\", \"verify\", \"password\", \"login\", \"transaction\", \"bank\",\n",
    "    \"security\", \"urgent\", \"click\", \"payment\", \"offer\", \"bonus\",\n",
    "    \"transfer\", \"discount\", \"alert\", \"update\", \"confirm\"\n",
    "]\n",
    "\n",
    "SUPPORTING_VERBS = [\n",
    "    \"check\", \"verify\", \"click\", \"read\", \"reply\", \"share\",\n",
    "    \"follow\", \"install\", \"open\", \"download\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f788f29",
   "metadata": {},
   "source": [
    "## Preprocessing & Pembobotan Bloom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa76c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def bloom_weight(text):\n",
    "    words = text.split()\n",
    "    score = 0\n",
    "    for w in words:\n",
    "        if w in BLOOM_VERBS:\n",
    "            score += 5\n",
    "        elif w in THEMATIC_TERMS:\n",
    "            score += 4\n",
    "        elif w in SUPPORTING_VERBS:\n",
    "            score += 3\n",
    "        else:\n",
    "            score += 1\n",
    "    return score / max(1, len(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9696527",
   "metadata": {},
   "source": [
    "## Fuzzy Logic untuk Penilaian Risiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb463b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_weight(bloom_score):\n",
    "    risk = ctrl.Antecedent(np.arange(0, 6, 0.1), 'risk')\n",
    "    weight = ctrl.Consequent(np.arange(0, 1.1, 0.1), 'weight')\n",
    "\n",
    "    risk['low'] = fuzz.trimf(risk.universe, [0, 0, 2])\n",
    "    risk['medium'] = fuzz.trimf(risk.universe, [1, 3, 5])\n",
    "    risk['high'] = fuzz.trimf(risk.universe, [3, 5, 5])\n",
    "\n",
    "    weight['low'] = fuzz.trimf(weight.universe, [0, 0, 0.5])\n",
    "    weight['medium'] = fuzz.trimf(weight.universe, [0.3, 0.6, 0.9])\n",
    "    weight['high'] = fuzz.trimf(weight.universe, [0.7, 1, 1])\n",
    "\n",
    "    rule1 = ctrl.Rule(risk['low'], weight['low'])\n",
    "    rule2 = ctrl.Rule(risk['medium'], weight['medium'])\n",
    "    rule3 = ctrl.Rule(risk['high'], weight['high'])\n",
    "\n",
    "    control = ctrl.ControlSystem([rule1, rule2, rule3])\n",
    "    sim = ctrl.ControlSystemSimulation(control)\n",
    "    sim.input['risk'] = bloom_score\n",
    "    sim.compute()\n",
    "    return sim.output['weight']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d1cf0",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68644413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    df1 = pd.read_csv(\"email_spam_indo.csv\")\n",
    "    df2 = pd.read_csv(\"spam.csv\", encoding_errors='ignore')\n",
    "\n",
    "    df1.columns = [c.lower() for c in df1.columns]\n",
    "    df2.columns = [c.lower() for c in df2.columns]\n",
    "\n",
    "    text_col = [c for c in df1.columns if 'pesan' in c or 'message' in c][0]\n",
    "    label_col = [c for c in df1.columns if 'kategori' in c or 'label' in c][0]\n",
    "\n",
    "    df = pd.concat([\n",
    "        df1[[text_col, label_col]],\n",
    "        df2[[df2.columns[1], df2.columns[0]]].rename(columns={df2.columns[1]: text_col, df2.columns[0]: label_col})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    df[\"clean\"] = df[text_col].apply(clean_text)\n",
    "    df[\"bloom\"] = df[\"clean\"].apply(bloom_weight)\n",
    "    df[\"fuzzy\"] = df[\"bloom\"].apply(fuzzy_weight)\n",
    "\n",
    "    print(\"‚úÖ Dataset berhasil dimuat:\", df.shape)\n",
    "    return df, text_col, label_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e33f17",
   "metadata": {},
   "source": [
    "## Pipeline Training Multi-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a527d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, text_col, label_col):\n",
    "    vectorizer = TfidfVectorizer(max_features=3000)\n",
    "    X = vectorizer.fit_transform(df[\"clean\"])\n",
    "    y = df[label_col].astype(str)\n",
    "\n",
    "    models = {\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        \"SVM\": SVC(kernel=\"linear\", probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"DNN\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X, y, cv=5)\n",
    "        results[name] = np.mean(scores)\n",
    "        print(f\"üîπ {name} | K-Fold Acc: {scores.mean():.4f}\")\n",
    "\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model = models[best_model_name].fit(X, y)\n",
    "\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(\"\\nüìà Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    joblib.dump(best_model, \"best_model.pkl\")\n",
    "    joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "    print(f\"üèÜ Model terbaik: {best_model_name} ({results[best_model_name]:.4f})\")\n",
    "    return best_model, vectorizer, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fa9ea",
   "metadata": {},
   "source": [
    "## Kriptografi Ringan (Hash Validasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c4c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash(prediction):\n",
    "    return hashlib.sha256(prediction.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d6aee",
   "metadata": {},
   "source": [
    "## Prediksi Email Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4c0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(text, model, vectorizer):\n",
    "    cleaned = clean_text(text)\n",
    "    X_input = vectorizer.transform([cleaned])\n",
    "    pred = model.predict(X_input)[0]\n",
    "    bloom = bloom_weight(cleaned)\n",
    "    fuzzy_score = fuzzy_weight(bloom)\n",
    "    hash_value = generate_hash(pred)\n",
    "    return pred, bloom, fuzzy_score, hash_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7403c",
   "metadata": {},
   "source": [
    "## Main Flow (Testing Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd8d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset berhasil dimuat: (8208, 5)\n",
      "üîπ Naive Bayes | K-Fold Acc: 0.8006\n",
      "üîπ SVM | K-Fold Acc: 0.9521\n",
      "üîπ Random Forest | K-Fold Acc: 0.8441\n",
      "üîπ DNN | K-Fold Acc: 0.9401\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "[[6074   19]\n",
      " [  67 2048]]\n",
      "\n",
      "üìà Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      6093\n",
      "        spam       0.99      0.97      0.98      2115\n",
      "\n",
      "    accuracy                           0.99      8208\n",
      "   macro avg       0.99      0.98      0.99      8208\n",
      "weighted avg       0.99      0.99      0.99      8208\n",
      "\n",
      "üèÜ Model terbaik: SVM (0.9521)\n",
      "\n",
      "=== Hasil Uji ===\n",
      "Prediksi: spam\n",
      "Bloom Score: 1.0\n",
      "Fuzzy Risk: 0.19\n",
      "SHA256 Hash: 4e388ab32b10dc8dbc7e ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df, text_col, label_col = load_dataset()\n",
    "    model, vectorizer, results = train_models(df, text_col, label_col)\n",
    "\n",
    "    test_text = \"Selamat! Anda memenangkan hadiah Rp50 juta! Klik tautan berikut.\"\n",
    "    pred, bloom, fuzzy, hashv = predict_email(test_text, model, vectorizer)\n",
    "\n",
    "    print(\"\\n=== Hasil Uji ===\")\n",
    "    print(\"Prediksi:\", pred)\n",
    "    print(\"Bloom Score:\", round(bloom, 2))\n",
    "    print(\"Fuzzy Risk:\", round(fuzzy, 2))\n",
    "    print(\"SHA256 Hash:\", hashv[:20], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
